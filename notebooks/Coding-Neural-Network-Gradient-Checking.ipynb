{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-family: Georgia; font-size:3em;color:#2462C0; font-style:bold\">\n",
    "Coding Neural Network - Gradient Checking</h1><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous notebook, [*Coding Neural Network - Forward Propagation and Backpropagation*](https://nbviewer.jupyter.org/github/ImadDabbura/blog-posts/blob/master/notebooks/Coding-Neural-Network-Forwad-Back-Propagation.ipynb), we implemented both forward propagation and backpropagation in `numpy`. However, implementing backpropagation from scratch is usually more prune to bugs/errors. Therefore, it's necessary before running the neural network on training data to check if our implementation of backpropagation is correct. Before we start, let's revisit what back-propagation is: We loop over the nodes in reverse topological order starting at the final node to compute the derivative of the cost with respect to each edge's node tail. In other words, we compute the derivative of cost function with respect to all parameters, i.e $\\frac{\\partial J}{\\partial \\theta}$ where $\\theta$ represents the parameters of the model.\n",
    "\n",
    "The way to test our implementation is by computing numerical gradients and compare it with gradients from backpropagation (analytical). There are two way of computing numerical gradients:\n",
    "- Right-hand form:\n",
    "$$\\frac{J(\\theta + \\epsilon) - J(\\theta)}{\\epsilon}\\tag{1}$$\n",
    "- Two-sided form (see figure 1):\n",
    "$$\\frac{J(\\theta + \\epsilon) - J(\\theta - \\epsilon)}{2 \\epsilon}\\tag{2}$$\n",
    "<p align=\"center\">\n",
    "<img src=\"posts_images/coding_nn_from_scratch/two_sided_gradients.png\" style=\"height: 400px; width: 700px\">\n",
    "<caption><center><u><b><font color=\"purple\"> Figure 1:</font></b></u> Two-sided numerical gradients<center></caption>\n",
    "</p>\n",
    "Two-sided form of approximating the derivative is closer than the right-hand form. Let's illustrate that with the following example using the function $f(x) = x^2$ by taking its derivative at $x = 3$.\n",
    "- Analytical derivative:\n",
    "$$\\nabla_x f(x) = 2x\\ \\Rightarrow\\nabla_x f(3) = 6$$\n",
    "- Two-sided numerical derivative:\n",
    "$$\\frac{(3 + 1e-2)^2 - (3 - 1e-2)^2}{2 * 1e-2} = 5.999999999999872$$\n",
    "- Right-hand numerical derivative:\n",
    "$$\\frac{(3 + 1e-2)^2 - 3^2}{1e-2} = 6.009999999999849$$\n",
    "As we see above, the difference between analytical derivative and two-sided numerical gradient is almost zero; however, the difference between analytical derivative and right-sided derivative is 0.01. Therefore, we'll use two-sided epsilon method to compute the numerical gradients.\n",
    "\n",
    "In addition, we'll normalize the difference between numerical. gradients and analytical gradients using the following formula:\n",
    "$$$$\n",
    "$$\\frac{\\|grad - grad_{approx}\\|_2}{\\|grad\\|_2 + \\|grad_{approx}\\|_2}\\tag{3}$$\n",
    "$$$$\n",
    "If the difference is $\\leq 10^{-7}$, then our implementation is fine; otherwise, we have a mistake somewhere and have to go back and revisit backpropagation code.\n",
    "\n",
    "Below are the steps needed to implement gradient checking:\n",
    "1. Pick random number of examples from training data to use it when computing both numerical and analytical gradients.\n",
    "    - Don't use all examples in the training data because gradient checking is very slow.\n",
    "2. Initialize parameters.\n",
    "3. Compute forward propagation and the cross-entropy cost.\n",
    "4. Compute the gradients using our back-propagation implementation.\n",
    "5. Compute the numerical gradients using the two-sided epsilon method.\n",
    "6. Compute the difference between numerical and analytical gradients.\n",
    "\n",
    "We'll be using functions we wrote in *\"Coding Neural Network - Forward Propagation and Backpropagation\"* notebook to initialize parameters, compute forward propagation and back-propagation as well as the cross-entropy cost.\n",
    "\n",
    "Let's first import the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Loading packages\n",
    "import sys\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import seaborn as sns\n",
    "\n",
    "sys.path.append(\"../scripts/\")\n",
    "from coding_neural_network_from_scratch import (initialize_parameters,\n",
    "                                                L_model_forward,\n",
    "                                                L_model_backward,\n",
    "                                                compute_cost)\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context(\"notebook\")\n",
    "plt.style.use(\"fivethirtyeight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((12288, 209), (1, 209))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the data\n",
    "train_dataset = h5py.File(\"../data/train_catvnoncat.h5\")\n",
    "X_train = np.array(train_dataset[\"train_set_x\"]).T\n",
    "y_train = np.array(train_dataset[\"train_set_y\"]).T\n",
    "X_train = X_train.reshape(-1, 209)\n",
    "y_train = y_train.reshape(-1, 209)\n",
    "\n",
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll write helper functions that faciltate converting parameters and gradients dictionaries into vectors and then re-convert them back to dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def dictionary_to_vector(params_dict):\n",
    "    \"\"\"\n",
    "    Roll a dictionary into a single vector.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    params_dict : dict\n",
    "        learned parameters.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params_vector : array\n",
    "        vector of all parameters concatenated.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for key in params_dict.keys():\n",
    "        new_vector = np.reshape(params_dict[key], (-1, 1))\n",
    "        if count == 0:\n",
    "            theta_vector = new_vector\n",
    "        else:\n",
    "            theta_vector = np.concatenate((theta_vector, new_vector))\n",
    "        count += 1\n",
    "\n",
    "    return theta_vector\n",
    "\n",
    "\n",
    "def vector_to_dictionary(vector, layers_dims):\n",
    "    \"\"\"\n",
    "    Unroll parameters vector to dictionary using layers dimensions.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    vector : array\n",
    "        parameters vector.\n",
    "    layers_dims : list or array_like\n",
    "        dimensions of each layer in the network.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    parameters : dict\n",
    "        dictionary storing all parameters.\n",
    "    \"\"\"\n",
    "    L = len(layers_dims)\n",
    "    parameters = {}\n",
    "    k = 0\n",
    "\n",
    "    for l in range(1, L):\n",
    "        # Create temp variable to store dimension used on each layer\n",
    "        w_dim = layers_dims[l] * layers_dims[l - 1]\n",
    "        b_dim = layers_dims[l]\n",
    "\n",
    "        # Create temp var to be used in slicing parameters vector\n",
    "        temp_dim = k + w_dim\n",
    "\n",
    "        # add parameters to the dictionary\n",
    "        parameters[\"W\" + str(l)] = vector[\n",
    "            k:temp_dim].reshape(layers_dims[l], layers_dims[l - 1])\n",
    "        parameters[\"b\" + str(l)] = vector[\n",
    "            temp_dim:temp_dim + b_dim].reshape(b_dim, 1)\n",
    "\n",
    "        k += w_dim + b_dim\n",
    "\n",
    "    return parameters\n",
    "\n",
    "\n",
    "def gradients_to_vector(gradients):\n",
    "    \"\"\"\n",
    "    Roll all gradients into a single vector containing only dW and db.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    gradients : dict\n",
    "        storing gradients of weights and biases for all layers: dA, dW, db.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    new_grads : array\n",
    "        vector of only dW and db gradients.\n",
    "    \"\"\"\n",
    "    # Get the number of indices for the gradients to iterate over\n",
    "    valid_grads = [key for key in gradients.keys()\n",
    "                   if not key.startswith(\"dA\")]\n",
    "    L = len(valid_grads)// 2\n",
    "    count = 0\n",
    "    \n",
    "    # Iterate over all gradients and append them to new_grads list\n",
    "    for l in range(1, L + 1):\n",
    "        if count == 0:\n",
    "            new_grads = gradients[\"dW\" + str(l)].reshape(-1, 1)\n",
    "            new_grads = np.concatenate(\n",
    "                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n",
    "        else:\n",
    "            new_grads = np.concatenate(\n",
    "                (new_grads, gradients[\"dW\" + str(l)].reshape(-1, 1)))\n",
    "            new_grads = np.concatenate(\n",
    "                (new_grads, gradients[\"db\" + str(l)].reshape(-1, 1)))\n",
    "        count += 1\n",
    "        \n",
    "    return new_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll write the gradient checking function that will compute the difference between the analytical and numerical gradients and tell us if our implementation of back-propagation is correct. We'll randomly choose 1 example to compute the difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def forward_prop_cost(X, parameters, Y, hidden_layers_activation_fn=\"tanh\"):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation and computes the cost.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    X : 2d-array\n",
    "        input data, shape: number of features x number of examples.\n",
    "    parameters : dict\n",
    "        parameters to use in forward prop.\n",
    "    Y : array\n",
    "        true \"label\", shape: 1 x number of examples.\n",
    "    hidden_layers_activation_fn : str\n",
    "        activation function to be used on hidden layers: \"tanh\", \"relu\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cost : float\n",
    "        cross-entropy cost.\n",
    "    \"\"\"\n",
    "    # Compute forward prop\n",
    "    AL, _ = L_model_forward(X, parameters, hidden_layers_activation_fn)\n",
    "\n",
    "    # Compute cost\n",
    "    cost = compute_cost(AL, Y)\n",
    "\n",
    "    return cost\n",
    "\n",
    "\n",
    "def gradient_check(\n",
    "        parameters, gradients, X, Y, layers_dims, epsilon=1e-7,\n",
    "        hidden_layers_activation_fn=\"tanh\"):\n",
    "    \"\"\"\n",
    "    Checks if back_prop computes correctly the gradient of the cost output by\n",
    "    forward_prop.\n",
    "    \n",
    "    Arguments\n",
    "    ---------\n",
    "    parameters : dict\n",
    "        storing all parameters to use in forward prop.\n",
    "    gradients : dict\n",
    "        gradients of weights and biases for all layers: dA, dW, db.\n",
    "    X : 2d-array\n",
    "        input data, shape: number of features x number of examples.\n",
    "    Y : array\n",
    "        true \"label\", shape: 1 x number of examples.\n",
    "    epsilon : \n",
    "        tiny shift to the input to compute approximate gradient.\n",
    "    layers_dims : list or array_like\n",
    "        dimensions of each layer in the network.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    difference : float\n",
    "        difference between approx gradient and back_prop gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    # Roll out parameters and gradients dictionaries\n",
    "    parameters_vector = dictionary_to_vector(parameters)\n",
    "    gradients_vector = gradients_to_vector(gradients)\n",
    "\n",
    "    # Create vector of zeros to be used with epsilon\n",
    "    grads_approx = np.zeros_like(parameters_vector)\n",
    "\n",
    "    for i in range(len(parameters_vector)):\n",
    "        # Compute cost of theta + epsilon\n",
    "        theta_plus = np.copy(parameters_vector)\n",
    "        theta_plus[i] = theta_plus[i] + epsilon\n",
    "        j_plus = forward_prop_cost(\n",
    "            X, vector_to_dictionary(theta_plus, layers_dims), Y,\n",
    "            hidden_layers_activation_fn)\n",
    "\n",
    "        # Compute cost of theta - epsilon\n",
    "        theta_minus = np.copy(parameters_vector)\n",
    "        theta_minus[i] = theta_minus[i] - epsilon\n",
    "        j_minus = forward_prop_cost(\n",
    "            X, vector_to_dictionary(theta_minus, layers_dims), Y,\n",
    "            hidden_layers_activation_fn)\n",
    "\n",
    "        # Compute numerical gradients\n",
    "        grads_approx[i] = (j_plus - j_minus) / (2 * epsilon)\n",
    "\n",
    "    # Compute the difference of numerical and analytical gradients\n",
    "    numerator = norm(gradients_vector - grads_approx)\n",
    "    denominator = norm(grads_approx) + norm(gradients_vector)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 10e-7:\n",
    "        print (\"\\033[31mThere is a mistake in back-propagation \" +\\\n",
    "               \"implementation. The difference is: {}\".format(difference))\n",
    "    else:\n",
    "        print (\"\\033[32mThere implementation of back-propagation is fine! \"+\\\n",
    "               \"The difference is: {}\".format(difference))\n",
    "\n",
    "    return difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mThere implementation of back-propagation is fine! The difference is: 3.0220555297630148e-09\n"
     ]
    }
   ],
   "source": [
    "# Set up neural network architecture\n",
    "layers_dims = [X_train.shape[0], 5, 5, 1]\n",
    "\n",
    "# Initialize parameters\n",
    "parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "# Randomly selecting 1 example from training data\n",
    "perms = np.random.permutation(X_train.shape[1])\n",
    "index = perms[:1]\n",
    "\n",
    "# Compute forward propagation\n",
    "AL, caches = L_model_forward(X_train[:, index], parameters, \"tanh\")\n",
    "\n",
    "# Compute analytical gradients\n",
    "gradients = L_model_backward(AL, y_train[:, index], caches, \"tanh\")\n",
    "\n",
    "# Compute difference of numerical and analytical gradients\n",
    "difference = gradient_check(parameters, gradients, X_train[:, index], y_train[:, index], layers_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Our implementation is correct :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"font-family: Georgia; font-size:2em;color:purple; font-style:bold\">\n",
    "Conclusion\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some key takeaways:\n",
    "- Two-sided numerical gradient approximates the analytical gradients more closely than right-side form.\n",
    "- Since gradient checking is very slow:\n",
    "    - Apply it on one or few training examples.\n",
    "    - Turn it off when training neural network after making sure that backpropagation's implementation is correct.\n",
    "- Gradient checking doesn't work when applying drop-out method. Use keep-prob = 1 to check gradient checking and then change it when training neural network.\n",
    "- Epsilon = 10e-7 is a common value used for the difference between analytical gradient and numerical gradient. If the difference is less than 10e-7 then the implementation of backpropagation is correct.\n",
    "- Thanks to *Deep Learning* frameworks such as Tensorflow and Pytorch, we may find ourselves rarely implement backpropagation because such frameworks compute that for us; however, it's a good practice to understand what happens under the hood to become a good Deep Learning practitioner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
